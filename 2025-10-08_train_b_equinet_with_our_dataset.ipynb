{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed164b70-fb54-4fd4-936c-a0f2e2354121",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3224e2-b6f3-49c6-a2dd-b02427ddde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import time\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.4\"\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec49c61-82cd-4100-8c23-c15163b25784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import Compressed, Uncompressed \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa0efa9-4223-4100-95d0-cee9bef10851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use with our MFISNet-style dataset\n",
    "import os\n",
    "from ISP_baseline.src.data_io import (\n",
    "    load_hdf5_to_dict,\n",
    "    load_cart_multifreq_dataset,\n",
    "    load_single_dir_slice,\n",
    "    load_multi_dir_slice,\n",
    "    get_multifreq_dset_dirs,\n",
    ")\n",
    "from ISP_baseline.src.datasets import (\n",
    "    convert_mfisnet_data_dict,\n",
    "    setup_tf_dataset,\n",
    ")\n",
    "from ISP_baseline.src.more_metrics import (\n",
    "    l2_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802b2e78-5208-4d2c-8e36-fb333ab4a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abdd75-21fc-41fb-9c42-f59498a696b6",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b885f2d4-dfbb-4189-8c87-7e49a3587b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlc_repo = os.path.join(\"/\", \"home-nfs\", \"oortsang\", \"rlc-repo\")\n",
    "dataset_dir = os.path.join(rlc_repo, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d81e86-55b9-4e42-959c-92554087e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4 # number of levels (even number)\n",
    "s = 12 # leaf size for N_x = 192\n",
    "# s = 5 # leaf size\n",
    "# s = 6\n",
    "r = 3 # rank -- not used??\n",
    "\n",
    "downsample_ratio = 4\n",
    "s = s // downsample_ratio\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "# NTRAIN = 21000\n",
    "# NTRAIN = 2000 # Reduced for debugging purposes\n",
    "NTRAIN = 1000 # for debugging purposes\n",
    "NVAL   = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc310b0c-1ab8-47d4-977b-8f49c1a4be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbar_str_list = [\"2.5\", \"5\", \"10\"]\n",
    "kbar_str_list = [\"2\", \"5\", \"10\"] # uuhhh I haven't prepared the 2.5 data\n",
    "nk = len(kbar_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efc367a-135f-443a-a83a-ea18202c2cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: x_vals(192,), q_cart(1000, 192, 192), sample_completion(1000,), d_rs(1000, 3, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "train_dirs = get_multifreq_dset_dirs(\n",
    "    \"train\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "train_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    train_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NTRAIN,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in train_mfisnet_dd.items()])}\")\n",
    "train_wb_dd = convert_mfisnet_data_dict(\n",
    "    train_mfisnet_dd,\n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio,\n",
    ")\n",
    "# eta     = train_wb_dd[\"eta\"]\n",
    "# scatter = train_wb_dd[\"scatter\"]\n",
    "\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "train_eta     = train_wb_dd[\"eta\"]\n",
    "train_scatter = train_wb_dd[\"scatter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c01ca3-aec6-471a-84d0-6252928e3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_dloader = setup_tf_dataset(\n",
    "    train_eta,\n",
    "    train_scatter,\n",
    "    batch_size=batch_size,\n",
    "    repeats=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852a9d28-a656-4155-babe-ae87c9a5a833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: x_vals(192,), q_cart(100, 192, 192), sample_completion(100,), d_rs(100, 3, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "val_dirs = get_multifreq_dset_dirs(\n",
    "    \"val\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "val_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    val_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NVAL,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in val_mfisnet_dd.items()])}\")\n",
    "val_wb_dd = convert_mfisnet_data_dict(\n",
    "    val_mfisnet_dd,\n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio,\n",
    ")\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "val_eta     = val_wb_dd[\"eta\"]\n",
    "val_scatter = val_wb_dd[\"scatter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0363485f-b93b-4d35-9928-1eee1e7a6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset, val_dloader = setup_tf_dataset(\n",
    "    val_eta,\n",
    "    val_scatter,\n",
    "    batch_size=100,\n",
    "    repeats=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8844ed2-507d-4ade-88c5-e8973cde4833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9921b616-aa69-4513-8c79-5b1dd9ed09c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 48, 48), (1000, 2, 2304, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eta.shape, train_scatter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1c82e-62c1-4fc8-9925-2cda81b1d1d9",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf578e5-79e7-4719-939e-76bb60252a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cnn_layers = 3\n",
    "N_cnn_channels = 6\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d44ab0-c9a6-4852-8547-18935d0f9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.load_mats_from_fp(os.path.join(\"tmp\", \"cart_and_rot_mats\", f\"mats_neta{neta}_nx{nx}.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b253d55-9982-4f3e-8d7c-1ad095a1ddef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2025-10-08 14:51:33.886303...\n",
      "CPU times: user 98.9 ms, sys: 128 ms, total: 226 ms\n",
      "Wall time: 392 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "print(f\"Starting at {datetime.now()}...\")\n",
    "cart_mat, r_index = utils.load_or_create_mats(\n",
    "    neta,\n",
    "    nx,\n",
    "    mats_dir=os.path.join(\"tmp\", \"cart_and_rot_mats\"),\n",
    "    mats_format=\"mats_neta{0}_nx{1}.npz\",\n",
    "    save_if_created=True,\n",
    ")\n",
    "\n",
    "# cart_mat = utils.SparsePolarToCartesian(neta, nx)\n",
    "# r_index  = utils.rotationindex(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfeae054-f6f8-48f1-b9b6-b6cd1bd96145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tornado, asyncio\n",
    "# print(tornado.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bdbdb02-1def-4feb-bc00-3b22d43a26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c49201-74e3-4b6e-8d94-6cc84ebd9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_module = Uncompressed.UncompressedModelFlexible(\n",
    "#     nx = nx,\n",
    "#     neta = neta,\n",
    "#     cart_mat = cart_mat,\n",
    "#     r_index = r_index,\n",
    "#     # New parameters\n",
    "#     nk=nk,\n",
    "#     N_cnn_layers=N_cnn_layers,\n",
    "#     N_cnn_channels=N_cnn_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "\n",
    "core_module = Compressed.CompressedModel(\n",
    "    L = L,\n",
    "    s = s,\n",
    "    r = r,\n",
    "    NUM_RESNET = 6,\n",
    "    NUM_CONV = 9,\n",
    "    cart_mat = cart_mat,\n",
    "    r_index = r_index,\n",
    ")\n",
    "\n",
    "# core_module = Uncompressed.UncompressedModel(\n",
    "#     nx = nx,\n",
    "#     neta = neta,\n",
    "#     cart_mat = cart_mat,\n",
    "#     r_index = r_index,\n",
    "#     # # Doesn't support these\n",
    "#     # nk=nk,\n",
    "#     # N_cnn_layers=N_cnn_layers,\n",
    "#     # N_cnn_channels=N_cnn_channels,\n",
    "#     # kernel_size=kernel_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bebee1c-19b6-4bd3-b5b6-e06606f106d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = train_scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f137de2-648d-412f-89e7-f39d048ad8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 14:51:51.662317: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 972.00MiB (rounded to 1019215872)requested by op \n",
      "2025-10-08 14:51:51.662397: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ****************************************************************************____****************____\n",
      "E1008 14:51:51.662410  432595 pjrt_stream_executor_client.cc:2839] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1019215872 bytes. [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1019215872 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m rng = jax.random.PRNGKey(\u001b[32m888\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m params = \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m param_count = \u001b[38;5;28msum\u001b[39m(x.size \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m jax.tree_util.tree_leaves(params))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mNumber of trainable parameters:\u001b[39m\u001b[33m'\u001b[39m, param_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/src/models.py:35\u001b[39m, in \u001b[36mDeterministicModel.initialize\u001b[39m\u001b[34m(self, rng)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize\u001b[39m(\u001b[38;5;28mself\u001b[39m, rng: Array):\n\u001b[32m     33\u001b[39m   \u001b[38;5;66;03m# TODO: Add a dtype object to ensure consistency of types.\u001b[39;00m\n\u001b[32m     34\u001b[39m   x = jnp.ones((\u001b[32m1\u001b[39m,) + \u001b[38;5;28mself\u001b[39m.input_shape)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 9 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Compressed.py:464\u001b[39m, in \u001b[36mCompressedModel.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03mThe forward pass of the CompressedModel:\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m  1. Processes three input channels through separate Fstar layers.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    461\u001b[39m \u001b[33;03m    jnp.ndarray: The final output of the model.\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# Process each channel separately using Fstar layers.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m y0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfstar_layer0\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m y1 = \u001b[38;5;28mself\u001b[39m.fstar_layer1(inputs[:, :, :, \u001b[32m1\u001b[39m])\n\u001b[32m    466\u001b[39m y2 = \u001b[38;5;28mself\u001b[39m.fstar_layer2(inputs[:, :, :, \u001b[32m2\u001b[39m])\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Compressed.py:400\u001b[39m, in \u001b[36mFstar.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;66;03m# Sequentially apply each H transformation.\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.Hs:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     y = \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# Reorder blocks using the switch indices. Apply the series of M modules in a residual (skip-connection) fashion.\u001b[39;00m\n\u001b[32m    403\u001b[39m y = y.take(\u001b[38;5;28mself\u001b[39m.switch_idx, axis=\u001b[32m1\u001b[39m).take(\u001b[38;5;28mself\u001b[39m.switch_idx, axis=\u001b[32m3\u001b[39m)\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Compressed.py:183\u001b[39m, in \u001b[36mH.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    180\u001b[39m y_im = y_im_1 + y_im_2 + y_im_3 + y_im_4\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Stack real and imaginary parts back into a single tensor.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m y = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_re\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_im\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# Reshape to merge the block dimensions back to the overall tensor shape.\u001b[39;00m\n\u001b[32m    186\u001b[39m n = m * \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Reconstructed spatial dimension.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:4477\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype)\u001b[39m\n\u001b[32m   4475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll input arrays must have the same shape.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4476\u001b[39m   new_arrays.append(expand_dims(a, axis))\n\u001b[32m-> \u001b[39m\u001b[32m4477\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:4648\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(arrays, axis, dtype)\u001b[39m\n\u001b[32m   4646\u001b[39m k = \u001b[32m16\u001b[39m\n\u001b[32m   4647\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4648\u001b[39m   arrays_out = [\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4649\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[32m   4650\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[32m0\u001b[39m]\n",
      "    \u001b[31m[... skipping hidden 11 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:1298\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1296\u001b[39m   \u001b[38;5;28mself\u001b[39m._handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dispatch.needs_check_special():\n\u001b[32m   1301\u001b[39m   out_arrays = results.disassemble_into_single_device_arrays()\n",
      "\u001b[31mXlaRuntimeError\u001b[39m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1019215872 bytes."
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a42193-62e2-409d-a820-cd33a7908704",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scatter[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac17a7-2a9d-4308-9c1f-f7cb357abe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cart_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48356746-9108-405d-9465-8049869dce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527d33e-3f98-44cb-a242-05dfbe50bf7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22872e4-f1f9-4878-92c8-f534a5ab6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "num_train_steps = NTRAIN * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + f\"/tmp/Uncompressed_nx_{nx}\"  #@param\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866784c-26de-4783-be04-a6d0705aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model,\n",
    "    rng=jax.random.PRNGKey(42),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904f8c1-995e-46d4-b640-69e3907ce5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dloader = train_dloader\n",
    "templates.run_train(\n",
    "    train_dataloader=train_dloader,\n",
    "    trainer=trainer,\n",
    "    workdir=workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps=10,\n",
    "    eval_dataloader=eval_dloader,\n",
    "    eval_every_steps = 100,\n",
    "    num_batches_per_eval = 1,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "            eval_monitors=(\"eval_rrmse_mean\", \"eval_rel_l2_mean\"),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=workdir,\n",
    "            options=ocp.CheckpointManagerOptions(\n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb202d0-07f8-462f-8a10-7cee8d30dca5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be1aeb-2023-45d9-a9b0-c045968ba402",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7d13a-fba7-4556-ab56-24315fe0db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTEST = 100\n",
    "\n",
    "test_dirs = get_multifreq_dset_dirs(\n",
    "    \"test\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "\n",
    "test_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    test_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NTEST,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in test_mfisnet_dd.items()])}\")\n",
    "test_wb_dd = convert_mfisnet_data_dict(\n",
    "    test_mfisnet_dd, \n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio\n",
    ")\n",
    "\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "test_eta     = test_wb_dd[\"eta\"] # [..., ::2, ::2]\n",
    "test_scatter = test_wb_dd[\"scatter\"] # [..., ::2, ::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0728c6-9803-4ee5-a349-f8807e78d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "test_dataset, test_dloader = setup_tf_dataset(\n",
    "    test_eta,\n",
    "    test_scatter,\n",
    "    # val_eta,\n",
    "    # val_scatter,\n",
    "    batch_size=test_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e3cfa-b713-4b2f-9bdd-d4565ab42e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = []\n",
    "validation_errors_rel_l2 = []\n",
    "validation_errors_rapsd = []\n",
    "pred_eta = np.zeros(test_eta.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "    metrics.mean_squared_error,\n",
    "    sum_axes=(-1, -2),\n",
    "    relative=True,\n",
    "    squared=False,\n",
    ")\n",
    "rel_l2 = functools.partial(\n",
    "    l2_error,\n",
    "    l2_axes=(-1, -2),\n",
    "    relative=True,\n",
    "    squared=False,\n",
    ")\n",
    "\n",
    "# for b, batch in enumerate(test_dloader):\n",
    "for b, batch in enumerate(val_dloader):\n",
    "    # pred = inference_fn(batch[0])\n",
    "    pred = inference_fn(batch[\"scatter\"])\n",
    "    batch_slice = np.s_[b*test_batch_size: (b+1)*test_batch_size, :, :]\n",
    "    pred_eta[batch_slice] = pred\n",
    "    true = batch[\"eta\"]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    validation_errors_rel_l2.append(rel_l2(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print(f\"Mean rel l2 error: {np.mean(validation_errors_rel_l2)*100:.3f}%\")\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7008cb5-e3bd-4c74-82c7-7cdc21b49fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb067c7-765d-453e-b605-e9bdc3f50417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "# test_data_path = os.path.join(\"data\", \"testdata\")\n",
    "\n",
    "# with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "#     # Read eta data, apply Gaussian blur, and reshape\n",
    "#     eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "#     blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "#     eta_test = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "    \n",
    "# # Loading and preprocessing scatter data (Lambda)\n",
    "# # with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "# with h5py.File(f'{test_data_path}/scatter.h5', 'r') as f:\n",
    "#     keys = natsort.natsorted(f.keys())\n",
    "\n",
    "#     # Process real part of scatter data\n",
    "#     tmp1 = f[keys[3]][:, :]\n",
    "#     tmp2 = f[keys[4]][:, :]\n",
    "#     tmp3 = f[keys[5]][:, :]\n",
    "#     scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "#     # Process imaginary part of scatter data\n",
    "#     tmp1 = f[keys[0]][:, :]\n",
    "#     tmp2 = f[keys[1]][:, :]\n",
    "#     tmp3 = f[keys[2]][:, :]\n",
    "#     scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "#     # Combine real and imaginary parts\n",
    "#     scatter_test = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "    \n",
    "# # Clean up temporary variables to free memory\n",
    "# del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c916a4-790c-4dd5-b205-00d18555a72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eed1d5-22f7-4b2f-a289-ad382e384903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9a0d4-6781-4481-8d15-331f02b06641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxisp-v3",
   "language": "python",
   "name": "jaxisp-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
