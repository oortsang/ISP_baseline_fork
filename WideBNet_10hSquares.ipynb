{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZDKhSAGaCrk2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.4\"\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZDKhSAGaCrk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import WideBNet \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd\n",
    "from ISP_baseline.src.more_metrics import (\n",
    "    l2_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761175107.896484 1422386 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "idx_flatten_to_morton = utils.flatten_to_morton_indices(L, s)\n",
    "idx_morton_to_flatten = utils.morton_to_flatten_indices(L, s)\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "# NTRAIN = 21000\n",
    "# NTRAIN = 2000\n",
    "NTRAIN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata'\n",
    "training_data_path = os.path.join(\"data\", \"traindata_L3s10_multifreq_square_3_5_10_h_freq_2.5_5_10\")\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{training_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :]) for i in range(NTRAIN)]).astype('float32')\n",
    "\n",
    "# Output normalization also...\n",
    "mean_eta, std_eta = np.mean(eta_re), np.std(eta_re)\n",
    "eta_re = (eta_re - mean_eta) / std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{training_data_path}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_re = scatter_re[:, idx_flatten_to_morton, :]\n",
    "    \n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_im = scatter_im[:, idx_flatten_to_morton, :]\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "\n",
    "mean0, std0 = np.mean(scatter[:,:,:,0]), np.std(scatter[:,:,:,0])\n",
    "mean1, std1 = np.mean(scatter[:,:,:,1]), np.std(scatter[:,:,:,1])\n",
    "mean2, std2 = np.mean(scatter[:,:,:,2]), np.std(scatter[:,:,:,2])\n",
    "\n",
    "scatter[:,:,:,0] -= mean0\n",
    "scatter[:,:,:,0] /= std0\n",
    "scatter[:,:,:,1] -= mean1\n",
    "scatter[:,:,:,1] /= std1\n",
    "scatter[:,:,:,2] -= mean2\n",
    "scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {\"eta\": eta_re}\n",
    "dict_data[\"scatter\"] = scatter\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = eval_dataloader = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M5F8kNMAGiTR"
   },
   "outputs": [],
   "source": [
    "core_module = WideBNet.WideBNetModel(\n",
    "     L = L, s = s, r = r, NUM_RESNET = 3, NUM_CNN = 3, idx_morton_to_flatten = idx_morton_to_flatten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:18:32.970944: W external/xla/xla/stream_executor/cuda/subprocess_compilation.cc:237] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 12.0\n",
      "2025-10-22 18:18:32.970959: W external/xla/xla/stream_executor/cuda/subprocess_compilation.cc:240] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2025-10-22 18:18:32.970998: W external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc:135] UNIMPLEMENTED: /usr/local/cuda/bin/ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1914061\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = NTRAIN * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + \"/tmp/WideBNet10squares\"  #@param\n",
    "if os.path.exists(workdir):\n",
    "    import shutil\n",
    "    shutil.rmtree(workdir)\n",
    "init_value = 5e-3\n",
    "transition_steps = 2000\n",
    "decay_rate = 0.95\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model, \n",
    "    rng=jax.random.PRNGKey(42), \n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.exponential_decay(\n",
    "            init_value = init_value, \n",
    "            transition_steps = transition_steps, \n",
    "            decay_rate = decay_rate, \n",
    "            staircase = True),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=trainer,\n",
    "    workdir=workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps = 10,\n",
    "    eval_dataloader = eval_dataloader,\n",
    "    eval_every_steps = 100,\n",
    "    num_batches_per_eval = 2,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "            eval_monitors=(\"eval_rrmse_mean\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=workdir,\n",
    "            options=ocp.CheckpointManagerOptions(\n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojUo2JDEHPCN"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RHlke6pGiHx"
   },
   "outputs": [],
   "source": [
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing perturbation data (eta)\n",
    "# test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "test_data_path = os.path.join(\"data\", \"testdata\")\n",
    "\n",
    "with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_test = np.stack([blur_fn(img) for img in eta_re]).astype('float32')\n",
    "eta_test = (eta_test - mean_eta) / std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "# with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "with h5py.File(f'{test_data_path}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:, :]\n",
    "    tmp2 = f[keys[4]][:, :]\n",
    "    tmp3 = f[keys[5]][:, :]\n",
    "    scatter_re = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_re = scatter_re[:, idx_flatten_to_morton, :]\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:, :]\n",
    "    tmp2 = f[keys[1]][:, :]\n",
    "    tmp3 = f[keys[2]][:, :]\n",
    "    scatter_im = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_im = scatter_im[:, idx_flatten_to_morton, :]\n",
    "    # Combine real and imaginary parts\n",
    "    scatter_test = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "\n",
    "scatter_test[:,:,:,0] -= mean0\n",
    "scatter_test[:,:,:,0] /= std0\n",
    "scatter_test[:,:,:,1] -= mean1\n",
    "scatter_test[:,:,:,1] /= std1\n",
    "scatter_test[:,:,:,2] -= mean2\n",
    "scatter_test[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 100\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((scatter_test, eta_test))\n",
    "test_dataset = test_dataset.batch(test_batch)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = [] \n",
    "validation_errors_rapsd = [] \n",
    "eta_pred = np.zeros(eta_test.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "        metrics.mean_squared_error,\n",
    "        sum_axes=(-1, -2),\n",
    "        relative=True,\n",
    "        squared=False,\n",
    "    )\n",
    "\n",
    "b = 0\n",
    "for batch in test_dataset:\n",
    "    pred = inference_fn(batch[0])\n",
    "    eta_pred[b*test_batch:(b+1)*test_batch,:,:] = pred\n",
    "    b += 1\n",
    "    true = batch[1]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_errors_rrmse = []\n",
    "# validation_errors_rel_l2 = []\n",
    "# validation_errors_rapsd = []\n",
    "# pred_eta = np.zeros(eta_test.shape)\n",
    "\n",
    "# rrmse = functools.partial(\n",
    "#     metrics.mean_squared_error,\n",
    "#     sum_axes=(-1, -2),\n",
    "#     relative=True,\n",
    "#     squared=False,\n",
    "# )\n",
    "# rel_l2 = functools.partial(\n",
    "#     l2_error,\n",
    "#     l2_axes=(-1, -2),\n",
    "#     relative=True,\n",
    "#     squared=False,\n",
    "# )\n",
    "\n",
    "# # for b, batch in enumerate(test_dloader):\n",
    "# for b, batch in enumerate(val_dloader):\n",
    "#     # pred = inference_fn(batch[0])\n",
    "#     pred = inference_fn(batch[\"scatter\"])\n",
    "#     batch_slice = np.s_[b*test_batch_size: (b+1)*test_batch_size, :, :]\n",
    "#     pred_eta[batch_slice] = pred\n",
    "#     true = batch[\"eta\"]\n",
    "#     validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "#     validation_errors_rel_l2.append(rel_l2(pred=pred, true=true))\n",
    "#     for i in range(true.shape[0]):\n",
    "#         validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "# print(f\"Mean rel l2 error: {np.mean(validation_errors_rel_l2)*100:.3f}%\")\n",
    "# print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "# print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_widebnet_10squares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "# NPLOT = 3\n",
    "# for kk in range(NPLOT):\n",
    "#     k = random.randint(0, test_batch)\n",
    "#     plt.subplot(NPLOT, 3, kk*NPLOT + 1)\n",
    "#     plt.imshow(batch[1][k,:,:])\n",
    "#     plt.xticks([]); plt.yticks([]); clim = plt.gci().get_clim();\n",
    "#     if kk == 0:\n",
    "#         plt.title('Exact', color='red')\n",
    "\n",
    "#     plt.subplot(NPLOT, 3, kk*NPLOT + 2)\n",
    "#     plt.imshow(pred[k,:,:])\n",
    "#     plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "#     if kk == 0:\n",
    "#         plt.title('Pred', color='red')\n",
    "\n",
    "#     plt.subplot(NPLOT, 3, kk*NPLOT + 3)\n",
    "#     plt.imshow(batch[1][k,:,:]-pred[k,:,:])\n",
    "#     plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "#     if kk == 0:\n",
    "#         plt.title('Error', color='red')                \n",
    "# plt.show()\n",
    "# #fig.savefig('widebnet10squares.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "jaxisp-env",
   "language": "python",
   "name": "jaxisp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
