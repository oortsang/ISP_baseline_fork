{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed164b70-fb54-4fd4-936c-a0f2e2354121",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3224e2-b6f3-49c6-a2dd-b02427ddde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import time\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "jax_device = jax.devices(\"gpu\")[0]\n",
    "jax.config.update(\"jax_default_device\", jax_device)\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "from clu import metric_writers\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec49c61-82cd-4100-8c23-c15163b25784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import Compressed, Uncompressed \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa0efa9-4223-4100-95d0-cee9bef10851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use with our MFISNet-style dataset\n",
    "import os\n",
    "from ISP_baseline.src.data_io import (\n",
    "    load_hdf5_to_dict,\n",
    "    load_cart_multifreq_dataset,\n",
    "    load_single_dir_slice,\n",
    "    load_multi_dir_slice,\n",
    "    get_multifreq_dset_dirs,\n",
    ")\n",
    "from ISP_baseline.src.datasets import (\n",
    "    convert_mfisnet_data_dict,\n",
    "    setup_tf_dataset,\n",
    ")\n",
    "from ISP_baseline.src.more_metrics import (\n",
    "    l2_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802b2e78-5208-4d2c-8e36-fb333ab4a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abdd75-21fc-41fb-9c42-f59498a696b6",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b885f2d4-dfbb-4189-8c87-7e49a3587b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlc_repo = os.path.join(\"/\", \"home-nfs\", \"oortsang\", \"rlc-repo\")\n",
    "dataset_dir = os.path.join(rlc_repo, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d81e86-55b9-4e42-959c-92554087e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4 # number of levels (even number)\n",
    "s = 12 # leaf size for N_x = 192\n",
    "# s = 5 # leaf size\n",
    "# s = 6\n",
    "r = 3 # rank -- not used??\n",
    "\n",
    "downsample_ratio = 1\n",
    "s = s // downsample_ratio\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "# NTRAIN = 21000\n",
    "# NTRAIN = 2000 # Reduced for debugging purposes\n",
    "NTRAIN = 1000 # for debugging purposes\n",
    "NVAL   = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc310b0c-1ab8-47d4-977b-8f49c1a4be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbar_str_list = [\"2.5\", \"5\", \"10\"]\n",
    "kbar_str_list = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "nk = len(kbar_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efc367a-135f-443a-a83a-ea18202c2cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: x_vals(192,), q_cart(1000, 192, 192), sample_completion(1000,), d_rs(1000, 10, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "train_dirs = get_multifreq_dset_dirs(\n",
    "    \"train\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "train_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    train_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NTRAIN,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in train_mfisnet_dd.items()])}\")\n",
    "train_wb_dd = convert_mfisnet_data_dict(\n",
    "    train_mfisnet_dd,\n",
    "    scatter_as_real=True,\n",
    "    real_imag_axis=2,\n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio,\n",
    ")\n",
    "# eta     = train_wb_dd[\"eta\"]\n",
    "# scatter = train_wb_dd[\"scatter\"]\n",
    "\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "train_eta     = train_wb_dd[\"eta\"]\n",
    "train_scatter = train_wb_dd[\"scatter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c01ca3-aec6-471a-84d0-6252928e3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_dloader = setup_tf_dataset(\n",
    "    train_eta,\n",
    "    train_scatter,\n",
    "    batch_size=batch_size,\n",
    "    repeats=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852a9d28-a656-4155-babe-ae87c9a5a833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: x_vals(192,), q_cart(100, 192, 192), sample_completion(100,), d_rs(100, 10, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "val_dirs = get_multifreq_dset_dirs(\n",
    "    \"val\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "val_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    val_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NVAL,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in val_mfisnet_dd.items()])}\")\n",
    "val_wb_dd = convert_mfisnet_data_dict(\n",
    "    val_mfisnet_dd,\n",
    "    scatter_as_real=True,\n",
    "    real_imag_axis=2,\n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio,\n",
    ")\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "val_eta     = val_wb_dd[\"eta\"]\n",
    "val_scatter = val_wb_dd[\"scatter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0363485f-b93b-4d35-9928-1eee1e7a6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset, val_dloader = setup_tf_dataset(\n",
    "    val_eta,\n",
    "    val_scatter,\n",
    "    batch_size=100,\n",
    "    repeats=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8844ed2-507d-4ade-88c5-e8973cde4833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9921b616-aa69-4513-8c79-5b1dd9ed09c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 192, 192), (1000, 36864, 2, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eta.shape, train_scatter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1c82e-62c1-4fc8-9925-2cda81b1d1d9",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf578e5-79e7-4719-939e-76bb60252a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cnn_layers = 3\n",
    "N_cnn_channels = 6\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d44ab0-c9a6-4852-8547-18935d0f9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.load_mats_from_fp(os.path.join(\"tmp\", \"cart_and_rot_mats\", f\"mats_neta{neta}_nx{nx}.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b253d55-9982-4f3e-8d7c-1ad095a1ddef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2025-10-19 23:24:43.939466...\n",
      "CPU times: user 83.9 ms, sys: 160 ms, total: 244 ms\n",
      "Wall time: 442 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "print(f\"Starting at {datetime.now()}...\")\n",
    "cart_mat, r_index = utils.load_or_create_mats(\n",
    "    neta,\n",
    "    nx,\n",
    "    mats_dir=os.path.join(\"tmp\", \"cart_and_rot_mats\"),\n",
    "    mats_format=\"mats_neta{0}_nx{1}.npz\",\n",
    "    save_if_created=True,\n",
    ")\n",
    "\n",
    "# cart_mat = utils.SparsePolarToCartesian(neta, nx)\n",
    "# r_index  = utils.rotationindex(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfeae054-f6f8-48f1-b9b6-b6cd1bd96145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tornado, asyncio\n",
    "# print(tornado.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bdbdb02-1def-4feb-bc00-3b22d43a26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c49201-74e3-4b6e-8d94-6cc84ebd9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_module = Uncompressed.UncompressedModelFlexible(\n",
    "#     nx = nx,\n",
    "#     neta = neta,\n",
    "#     cart_mat = cart_mat,\n",
    "#     r_index = r_index,\n",
    "#     # New parameters\n",
    "#     nk=nk,\n",
    "#     N_cnn_layers=N_cnn_layers,\n",
    "#     N_cnn_channels=N_cnn_channels,\n",
    "#     kernel_size=kernel_size,\n",
    "# )\n",
    "\n",
    "core_module = Compressed.CompressedModelFlexible(\n",
    "    L = L,\n",
    "    s = s,\n",
    "    r = r,\n",
    "    # NUM_RESNET = 6,\n",
    "    # NUM_CONV = 9,\n",
    "    cart_mat = cart_mat,\n",
    "    r_index = r_index,\n",
    "    nk=nk,\n",
    "    N_resnet_layers=6,\n",
    "    # N_resnet_channels=3,\n",
    "    N_cnn_layers=9,\n",
    "    N_cnn_channels=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bebee1c-19b6-4bd3-b5b6-e06606f106d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = train_scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f137de2-648d-412f-89e7-f39d048ad8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 270595\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a42193-62e2-409d-a820-cd33a7908704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36864, 2, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scatter[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bac17a7-2a9d-4308-9c1f-f7cb357abe7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jax.experimental.sparse.bcoo.BCOO"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cart_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48356746-9108-405d-9465-8049869dce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527d33e-3f98-44cb-a242-05dfbe50bf7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d22872e4-f1f9-4878-92c8-f534a5ab6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "num_train_steps = NTRAIN * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + f\"/tmp/2025-10-19_compressed_nx_{nx}_nk_{nk}\"  #@param\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0866784c-26de-4783-be04-a6d0705aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model,\n",
    "    rng=jax.random.PRNGKey(42),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "325e32c8-bdd6-4f8b-aa8e-afdceb7203ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage associated with a single forward pass...\n",
      "RAM Used (MB): 51133\n",
      "VRAM (MB): 43731 free of 43776 total (within preallocation); usage is currently 44 and peaked at 352\n",
      "RAM Used (MB): 52372\n",
      "VRAM (MB): 43729 free of 43776 total (within preallocation); usage is currently 47 and peaked at 2767\n"
     ]
    }
   ],
   "source": [
    "untrained_inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trainer.train_state, core_module\n",
    ")\n",
    "\n",
    "print(f\"Memory usage associated with a single forward pass...\")\n",
    "_ = utils.get_memory_info_jax(device=jax_device, print_msg=True)\n",
    "# pre-run:     244\n",
    "# after 16:   2694\n",
    "# after 32:   5286\n",
    "# after 64:  10469\n",
    "# after 128: 20865\n",
    "trial_batch_size = 16\n",
    "\n",
    "# first_batch = next(train_dloader)\n",
    "trial_input = train_wb_dd[\"scatter\"][:trial_batch_size]\n",
    "trial_output = untrained_inference_fn(trial_input)\n",
    "\n",
    "_ = utils.get_memory_info_jax(device=jax_device, print_msg=True)\n",
    "\n",
    "# Okay, so running the forward pass is not a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a904f8c1-995e-46d4-b640-69e3907ce5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM Used (MB): 52372\n",
      "VRAM (MB): 43729 free of 43776 total (within preallocation); usage is currently 47 and peaked at 2767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7288fd94a64994b2aec69094e87b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31250 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 23:26:51.266150: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 40.56GiB (43554190758 bytes) by rematerialization; only reduced to 46.75GiB (50193017580 bytes), down from 60.15GiB (64586297936 bytes) originally\n",
      "2025-10-19 23:27:15.751165: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 46.07GiB (rounded to 49462839296)requested by op \n",
      "2025-10-19 23:27:15.753564: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] *___________________________________________________________________________________________________\n",
      "E1019 23:27:15.754011 3132302 pjrt_stream_executor_client.cc:2839] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49462839208 bytes. [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49462839208 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# eval_dloader = train_dloader\u001b[39;00m\n\u001b[32m      4\u001b[39m eval_dloader = val_dloader\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtemplates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_train_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_writer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_writers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_default_writer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_aggregation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_every_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_batches_per_eval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTqdmProgressBar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtotal_train_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_monitors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_monitors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_rrmse_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_rel_l2_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainStateCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCheckpointManagerOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_interval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_ckpt_to_keep\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/swirl_dynamics/templates/train.py:112\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(train_dataloader, trainer, workdir, total_train_steps, metric_aggregation_steps, eval_dataloader, eval_every_steps, num_batches_per_eval, run_sanity_eval_batch, metric_writer, callbacks)\u001b[39m\n\u001b[32m    109\u001b[39m   callback.on_train_batches_begin(trainer)\n\u001b[32m    111\u001b[39m num_steps = \u001b[38;5;28mmin\u001b[39m(total_train_steps - cur_step, metric_aggregation_steps)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m train_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m.compute()\n\u001b[32m    113\u001b[39m cur_step += num_steps\n\u001b[32m    114\u001b[39m metric_writer.write_scalars(cur_step, train_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/swirl_dynamics/templates/trainers.py:145\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self, batch_iter, num_steps)\u001b[39m\n\u001b[32m    139\u001b[39m train_rng, preproc_rng = \u001b[38;5;28mself\u001b[39m.get_train_rng(step=cur_step, num=\u001b[32m2\u001b[39m)\n\u001b[32m    140\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.preprocess_train_batch(\n\u001b[32m    141\u001b[39m     batch_data=\u001b[38;5;28mnext\u001b[39m(batch_iter),\n\u001b[32m    142\u001b[39m     step=cur_step,\n\u001b[32m    143\u001b[39m     rng=preproc_rng,\n\u001b[32m    144\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28mself\u001b[39m.train_state, metrics_update = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compiled_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# NOTE: In a distributed setting, the metrics are expected to be\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# gathered and reduced inside the `train_step` function. See\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# `clu_metrics.Collection.gather_from_model_output()` for a convenient\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# way to do this.\u001b[39;00m\n\u001b[32m    152\u001b[39m metrics_update = \u001b[38;5;28mself\u001b[39m._maybe_unreplicate(metrics_update)\n",
      "    \u001b[31m[... skipping hidden 5 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-v3/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:1298\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1296\u001b[39m   \u001b[38;5;28mself\u001b[39m._handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dispatch.needs_check_special():\n\u001b[32m   1301\u001b[39m   out_arrays = results.disassemble_into_single_device_arrays()\n",
      "\u001b[31mXlaRuntimeError\u001b[39m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49462839208 bytes."
     ]
    }
   ],
   "source": [
    "utils.get_memory_info_jax(jax_device, print_msg=True)\n",
    "\n",
    "# eval_dloader = train_dloader\n",
    "eval_dloader = val_dloader\n",
    "templates.run_train(\n",
    "    train_dataloader=train_dloader,\n",
    "    trainer=trainer,\n",
    "    workdir=workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps=10,\n",
    "    eval_dataloader=eval_dloader,\n",
    "    eval_every_steps = 100,\n",
    "    num_batches_per_eval = 1,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "            eval_monitors=(\"eval_rrmse_mean\", \"eval_rel_l2_mean\"),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=workdir,\n",
    "            options=ocp.CheckpointManagerOptions(\n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb202d0-07f8-462f-8a10-7cee8d30dca5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be1aeb-2023-45d9-a9b0-c045968ba402",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7d13a-fba7-4556-ab56-24315fe0db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTEST = 100\n",
    "\n",
    "test_dirs = get_multifreq_dset_dirs(\n",
    "    \"test\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "\n",
    "test_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    test_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NTEST,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in test_mfisnet_dd.items()])}\")\n",
    "test_wb_dd = convert_mfisnet_data_dict(\n",
    "    test_mfisnet_dd, \n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio\n",
    ")\n",
    "\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "test_eta     = test_wb_dd[\"eta\"] # [..., ::2, ::2]\n",
    "test_scatter = test_wb_dd[\"scatter\"] # [..., ::2, ::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0728c6-9803-4ee5-a349-f8807e78d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "test_dataset, test_dloader = setup_tf_dataset(\n",
    "    test_eta,\n",
    "    test_scatter,\n",
    "    # val_eta,\n",
    "    # val_scatter,\n",
    "    batch_size=test_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e3cfa-b713-4b2f-9bdd-d4565ab42e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = []\n",
    "validation_errors_rel_l2 = []\n",
    "validation_errors_rapsd = []\n",
    "pred_eta = np.zeros(test_eta.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "    metrics.mean_squared_error,\n",
    "    sum_axes=(-1, -2),\n",
    "    relative=True,\n",
    "    squared=False,\n",
    ")\n",
    "rel_l2 = functools.partial(\n",
    "    l2_error,\n",
    "    l2_axes=(-1, -2),\n",
    "    relative=True,\n",
    "    squared=False,\n",
    ")\n",
    "\n",
    "# for b, batch in enumerate(test_dloader):\n",
    "for b, batch in enumerate(val_dloader):\n",
    "    # pred = inference_fn(batch[0])\n",
    "    pred = inference_fn(batch[\"scatter\"])\n",
    "    batch_slice = np.s_[b*test_batch_size: (b+1)*test_batch_size, :, :]\n",
    "    pred_eta[batch_slice] = pred\n",
    "    true = batch[\"eta\"]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    validation_errors_rel_l2.append(rel_l2(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print(f\"Mean rel l2 error: {np.mean(validation_errors_rel_l2)*100:.3f}%\")\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7008cb5-e3bd-4c74-82c7-7cdc21b49fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb067c7-765d-453e-b605-e9bdc3f50417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "# test_data_path = os.path.join(\"data\", \"testdata\")\n",
    "\n",
    "# with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "#     # Read eta data, apply Gaussian blur, and reshape\n",
    "#     eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "#     blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "#     eta_test = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "    \n",
    "# # Loading and preprocessing scatter data (Lambda)\n",
    "# # with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "# with h5py.File(f'{test_data_path}/scatter.h5', 'r') as f:\n",
    "#     keys = natsort.natsorted(f.keys())\n",
    "\n",
    "#     # Process real part of scatter data\n",
    "#     tmp1 = f[keys[3]][:, :]\n",
    "#     tmp2 = f[keys[4]][:, :]\n",
    "#     tmp3 = f[keys[5]][:, :]\n",
    "#     scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "#     # Process imaginary part of scatter data\n",
    "#     tmp1 = f[keys[0]][:, :]\n",
    "#     tmp2 = f[keys[1]][:, :]\n",
    "#     tmp3 = f[keys[2]][:, :]\n",
    "#     scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "#     # Combine real and imaginary parts\n",
    "#     scatter_test = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "    \n",
    "# # Clean up temporary variables to free memory\n",
    "# del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c916a4-790c-4dd5-b205-00d18555a72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eed1d5-22f7-4b2f-a289-ad382e384903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9a0d4-6781-4481-8d15-331f02b06641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxisp-v3",
   "language": "python",
   "name": "jaxisp-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
