{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDKhSAGaCrk2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDKhSAGaCrk2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import WideBNet \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "idx_flatten_to_morton = utils.flatten_to_morton_indices(L, s)\n",
    "idx_morton_to_flatten = utils.morton_to_flatten_indices(L, s)\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "NTRAIN = 21000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata'\n",
    "\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{training_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :]) for i in range(NTRAIN)]).astype('float32')\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{training_data_path}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_re = scatter_re[:, idx_flatten_to_morton, :]\n",
    "    \n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_im = scatter_im[:, idx_flatten_to_morton, :]\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {\"eta\": eta_re}\n",
    "dict_data[\"scatter\"] = scatter\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = eval_dataloader = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5F8kNMAGiTR"
   },
   "outputs": [],
   "source": [
    "core_module = WideBNet.WideBNetModel(\n",
    "     L = L, s = s, r = r, NUM_RESNET = 3, NUM_CNN = 3, idx_morton_to_flatten = idx_morton_to_flatten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = 21000 * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + \"/tmp/WideBNet10squares\"  #@param\n",
    "init_value = 5e-3\n",
    "transition_steps = 2000\n",
    "decay_rate = 0.95\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model, \n",
    "    rng=jax.random.PRNGKey(42), \n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.exponential_decay(\n",
    "            init_value = init_value, \n",
    "            transition_steps = transition_steps, \n",
    "            decay_rate = decay_rate, \n",
    "            staircase = True),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=trainer,\n",
    "    workdir=workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps = 100,\n",
    "    eval_dataloader = eval_dataloader,\n",
    "    eval_every_steps = 5000,\n",
    "    num_batches_per_eval = 2,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "            eval_monitors=(\"eval_rrmse_mean\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=workdir,\n",
    "            options=ocp.CheckpointManagerOptions(\n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojUo2JDEHPCN"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RHlke6pGiHx"
   },
   "outputs": [],
   "source": [
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing perturbation data (eta)\n",
    "test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "\n",
    "with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_test = np.stack([blur_fn(img) for img in eta_re]).astype('float32')\n",
    "\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:, :]\n",
    "    tmp2 = f[keys[4]][:, :]\n",
    "    tmp3 = f[keys[5]][:, :]\n",
    "    scatter_re = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_re = scatter_re[:, idx_flatten_to_morton, :]\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:, :]\n",
    "    tmp2 = f[keys[1]][:, :]\n",
    "    tmp3 = f[keys[2]][:, :]\n",
    "    scatter_im = np.stack((tmp3, tmp2, tmp1), axis=-1)\n",
    "    scatter_im = scatter_im[:, idx_flatten_to_morton, :]\n",
    "    # Combine real and imaginary parts\n",
    "    scatter_test = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "    \n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 100\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((scatter_test, eta_test))\n",
    "test_dataset = test_dataset.batch(test_batch)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = [] \n",
    "validation_errors_rapsd = [] \n",
    "eta_pred = np.zeros(eta_test.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "        metrics.mean_squared_error,\n",
    "        sum_axes=(-1, -2),\n",
    "        relative=True,\n",
    "        squared=False,\n",
    "    )\n",
    "\n",
    "b = 0\n",
    "for batch in test_dataset:\n",
    "    pred = inference_fn(batch[0])\n",
    "    eta_pred[b*test_batch:(b+1)*test_batch,:,:] = pred\n",
    "    b += 1\n",
    "    true = batch[1]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_widebnet_10squares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "NPLOT = 3\n",
    "for kk in range(NPLOT):\n",
    "    k = random.randint(0, test_batch)\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 1)\n",
    "    plt.imshow(batch[1][k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); clim = plt.gci().get_clim();\n",
    "    if kk == 0:\n",
    "        plt.title('Exact', color='red')\n",
    "\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 2)\n",
    "    plt.imshow(pred[k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "    if kk == 0:\n",
    "        plt.title('Pred', color='red')\n",
    "\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 3)\n",
    "    plt.imshow(batch[1][k,:,:]-pred[k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "    if kk == 0:\n",
    "        plt.title('Error', color='red')                \n",
    "plt.show()\n",
    "#fig.savefig('widebnet10squares.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
