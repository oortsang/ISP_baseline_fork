{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZDKhSAGaCrk2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.5\"\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZDKhSAGaCrk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import Compressed \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "# NTRAIN = 21000\n",
    "NTRAIN = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata'\n",
    "training_data_path = os.path.join(\"data\", \"traindata_L3s10_multifreq_square_3_5_10_h_freq_2.5_5_10\")\n",
    "\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{training_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :].T) for i in range(NTRAIN)]).astype('float32')\n",
    "    \n",
    "#mean_eta, std_eta = np.mean(eta_re), np.std(eta_re)\n",
    "#eta_re -= mean_eta\n",
    "#eta_re /= std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{training_data_path}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "#mean0, std0 = np.mean(scatter[:,:,:,0]), np.std(scatter[:,:,:,0])\n",
    "#mean1, std1 = np.mean(scatter[:,:,:,1]), np.std(scatter[:,:,:,1])\n",
    "#mean2, std2 = np.mean(scatter[:,:,:,2]), np.std(scatter[:,:,:,2])\n",
    "#\n",
    "#scatter[:,:,:,0] -= mean0\n",
    "#scatter[:,:,:,0] /= std0\n",
    "#scatter[:,:,:,1] -= mean1\n",
    "#scatter[:,:,:,1] /= std1\n",
    "#scatter[:,:,:,2] -= mean2\n",
    "#scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {\"eta\": eta_re}\n",
    "dict_data[\"scatter\"] = scatter\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = eval_dataloader = dataset.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.1 ms, sys: 180 ms, total: 250 ms\n",
      "Wall time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cart_mat, r_index = utils.load_or_create_mats(\n",
    "    neta,\n",
    "    nx,\n",
    "    mats_dir=os.path.join(\"tmp\", \"cart_and_rot_mats\"),\n",
    "    mats_format=\"mats_neta{0}_nx{1}.npz\",\n",
    "    save_if_created=True,\n",
    ")\n",
    "\n",
    "# cart_mat = utils.SparsePolarToCartesian(neta, nx)\n",
    "# r_index = utils.rotationindex(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M5F8kNMAGiTR"
   },
   "outputs": [],
   "source": [
    "core_module = Compressed.CompressedModel(\n",
    "     L = L, s = s, r = r, NUM_RESNET = 6, cart_mat = cart_mat, r_index = r_index, NUM_CONV = 9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cart_mat, r_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 73594\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = 21000 * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + \"/tmp/Compressed10squares\"  #@param\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model, \n",
    "    rng=jax.random.PRNGKey(42), \n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939eee5ace52419e9d79a82bd2f6a64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131250 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/2000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/2000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/100 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/100) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/4000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/4000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/2000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/2000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/4000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/4000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/4000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/4000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/10000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/10000) to end with \".orbax-checkpoint-tmp\".\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=trainer,\n",
    "    workdir=workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps=100,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    eval_every_steps = 1000,\n",
    "    num_batches_per_eval = 1,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "            eval_monitors=(\"eval_rrmse_mean\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=workdir,\n",
    "            options=ocp.CheckpointManagerOptions(\n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojUo2JDEHPCN"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "test_data_path = os.path.join(\"data\", \"testdata\")\n",
    "\n",
    "with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_test = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "    \n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:, :]\n",
    "    tmp2 = f[keys[4]][:, :]\n",
    "    tmp3 = f[keys[5]][:, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:, :]\n",
    "    tmp2 = f[keys[1]][:, :]\n",
    "    tmp3 = f[keys[2]][:, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter_test = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 100\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((scatter_test, eta_test))\n",
    "test_dataset = test_dataset.batch(test_batch)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = [] \n",
    "validation_errors_rapsd = [] \n",
    "eta_pred = np.zeros(eta_test.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "        metrics.mean_squared_error,\n",
    "        sum_axes=(-1, -2),\n",
    "        relative=True,\n",
    "        squared=False,\n",
    "    )\n",
    "\n",
    "b = 0\n",
    "for batch in test_dataset:\n",
    "    print(b)\n",
    "    pred = inference_fn(batch[0])\n",
    "    eta_pred[b*test_batch:(b+1)*test_batch,:,:] = pred\n",
    "    b += 1\n",
    "    true = batch[1]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_compressed_10squares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "NPLOT = 3\n",
    "for kk in range(NPLOT):\n",
    "    k = random.randint(0, test_batch)\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 1)\n",
    "    plt.imshow(batch[1][k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); clim = plt.gci().get_clim();\n",
    "    if kk == 0:\n",
    "        plt.title('Exact', color='red')\n",
    "\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 2)\n",
    "    plt.imshow(pred[k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "    if kk == 0:\n",
    "        plt.title('Pred', color='red')\n",
    "\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 3)\n",
    "    plt.imshow(batch[1][k,:,:]-pred[k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "    if kk == 0:\n",
    "        plt.title('Error', color='red')                \n",
    "plt.show()\n",
    "#fig.savefig('compressed10squares.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "jaxisp-env",
   "language": "python",
   "name": "jaxisp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
