{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZDKhSAGaCrk2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.45\"\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZDKhSAGaCrk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import Compressed \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "# NTRAIN = 21000\n",
    "NTRAIN = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata'\n",
    "training_data_path = os.path.join(\"data\", \"traindata_L3s10_multifreq_square_3_5_10_h_freq_2.5_5_10\")\n",
    "\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{training_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :].T) for i in range(NTRAIN)]).astype('float32')\n",
    "    \n",
    "#mean_eta, std_eta = np.mean(eta_re), np.std(eta_re)\n",
    "#eta_re -= mean_eta\n",
    "#eta_re /= std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{training_data_path}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "#mean0, std0 = np.mean(scatter[:,:,:,0]), np.std(scatter[:,:,:,0])\n",
    "#mean1, std1 = np.mean(scatter[:,:,:,1]), np.std(scatter[:,:,:,1])\n",
    "#mean2, std2 = np.mean(scatter[:,:,:,2]), np.std(scatter[:,:,:,2])\n",
    "#\n",
    "#scatter[:,:,:,0] -= mean0\n",
    "#scatter[:,:,:,0] /= std0\n",
    "#scatter[:,:,:,1] -= mean1\n",
    "#scatter[:,:,:,1] /= std1\n",
    "#scatter[:,:,:,2] -= mean2\n",
    "#scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {\"eta\": eta_re}\n",
    "dict_data[\"scatter\"] = scatter\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = eval_dataloader = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 80, 80), (2000, 6400, 2, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_re.shape, scatter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.3 ms, sys: 286 ms, total: 341 ms\n",
      "Wall time: 5.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cart_mat, r_index = utils.load_or_create_mats(\n",
    "    neta,\n",
    "    nx,\n",
    "    mats_dir=os.path.join(\"tmp\", \"cart_and_rot_mats\"),\n",
    "    mats_format=\"mats_neta{0}_nx{1}.npz\",\n",
    "    save_if_created=True,\n",
    ")\n",
    "\n",
    "# cart_mat = utils.SparsePolarToCartesian(neta, nx)\n",
    "# r_index = utils.rotationindex(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M5F8kNMAGiTR"
   },
   "outputs": [],
   "source": [
    "core_module = Compressed.CompressedModel(\n",
    "     L = L, s = s, r = r, NUM_RESNET = 6, cart_mat = cart_mat, r_index = r_index, NUM_CONV = 9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cart_mat, r_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 17:12:02.491560: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng3{k11=0} for conv %cudnn-conv.1 = (f32[1,6,80,80]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.39, %bitcast.46), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Compressed.py\" source_line=473}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-10 17:12:02.507950: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.016491163s\n",
      "Trying algorithm eng3{k11=0} for conv %cudnn-conv.1 = (f32[1,6,80,80]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.39, %bitcast.46), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", metadata={op_name=\"jit(conv_general_dilated)/jit(main)/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Compressed.py\" source_line=473}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 73594\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = 21000 * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + \"/tmp/Compressed10squares\"  #@param\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model, \n",
    "    rng=jax.random.PRNGKey(42), \n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734b061f2cd647c7879badad4a7dddf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131250 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/6000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/12000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/12000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/10000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/10000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/14000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/14000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/12000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/12000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/8000) to end with \".orbax-checkpoint-tmp\".\n",
      "WARNING:absl:Path /share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/10000 could not be identified as a temporary checkpoint path using <class 'orbax.checkpoint._src.path.atomicity.AtomicRenameTemporaryPath'>. Got error: Expected AtomicRenameTemporaryPath (/share/data/willett-group/oortsang/ISP_baseline_fork/tmp/Compressed10squares/checkpoints/10000) to end with \".orbax-checkpoint-tmp\".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtemplates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_train_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_writer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_writers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_default_writer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_aggregation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_every_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_batches_per_eval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTqdmProgressBar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtotal_train_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_monitors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_monitors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_rrmse_mean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemplates\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainStateCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mocp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCheckpointManagerOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                \u001b[49m\u001b[43msave_interval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_ckpt_to_keep\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/swirl_dynamics/templates/train.py:112\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(train_dataloader, trainer, workdir, total_train_steps, metric_aggregation_steps, eval_dataloader, eval_every_steps, num_batches_per_eval, run_sanity_eval_batch, metric_writer, callbacks)\u001b[39m\n\u001b[32m    109\u001b[39m   callback.on_train_batches_begin(trainer)\n\u001b[32m    111\u001b[39m num_steps = \u001b[38;5;28mmin\u001b[39m(total_train_steps - cur_step, metric_aggregation_steps)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m train_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m.compute()\n\u001b[32m    113\u001b[39m cur_step += num_steps\n\u001b[32m    114\u001b[39m metric_writer.write_scalars(cur_step, train_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/swirl_dynamics/templates/trainers.py:139\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self, batch_iter, num_steps)\u001b[39m\n\u001b[32m    137\u001b[39m cur_step = step0 + step\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.profiler.StepTraceAnnotation(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, step_num=cur_step):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m   train_rng, preproc_rng = \u001b[38;5;28mself\u001b[39m.get_train_rng(step=cur_step, num=\u001b[32m2\u001b[39m)\n\u001b[32m    140\u001b[39m   batch = \u001b[38;5;28mself\u001b[39m.preprocess_train_batch(\n\u001b[32m    141\u001b[39m       batch_data=\u001b[38;5;28mnext\u001b[39m(batch_iter),\n\u001b[32m    142\u001b[39m       step=cur_step,\n\u001b[32m    143\u001b[39m       rng=preproc_rng,\n\u001b[32m    144\u001b[39m   )\n\u001b[32m    145\u001b[39m   \u001b[38;5;28mself\u001b[39m.train_state, metrics_update = \u001b[38;5;28mself\u001b[39m._compiled_train_step(\n\u001b[32m    146\u001b[39m       \u001b[38;5;28mself\u001b[39m.train_state, batch, \u001b[38;5;28mself\u001b[39m._maybe_split(train_rng)\n\u001b[32m    147\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/jax/_src/array.py:391\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_addressable\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dispatch.is_single_device_sharding(\u001b[38;5;28mself\u001b[39m.sharding) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated:\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (sl \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._chunk_iter(\u001b[32m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_unstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.sharding, PmapSharding):\n\u001b[32m    393\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.shape[\u001b[32m0\u001b[39m]))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=trainer,\n",
    "    workdir=workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps=100,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    eval_every_steps = 1000,\n",
    "    num_batches_per_eval = 1,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "            eval_monitors=(\"eval_rrmse_mean\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=workdir,\n",
    "            options=ocp.CheckpointManagerOptions(\n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojUo2JDEHPCN"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "test_data_path = os.path.join(\"data\", \"testdata\")\n",
    "\n",
    "with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_test = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "    \n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:, :]\n",
    "    tmp2 = f[keys[4]][:, :]\n",
    "    tmp3 = f[keys[5]][:, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:, :]\n",
    "    tmp2 = f[keys[1]][:, :]\n",
    "    tmp3 = f[keys[2]][:, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter_test = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 100\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((scatter_test, eta_test))\n",
    "test_dataset = test_dataset.batch(test_batch)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = [] \n",
    "validation_errors_rapsd = [] \n",
    "eta_pred = np.zeros(eta_test.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "        metrics.mean_squared_error,\n",
    "        sum_axes=(-1, -2),\n",
    "        relative=True,\n",
    "        squared=False,\n",
    "    )\n",
    "\n",
    "b = 0\n",
    "for batch in test_dataset:\n",
    "    print(b)\n",
    "    pred = inference_fn(batch[0])\n",
    "    eta_pred[b*test_batch:(b+1)*test_batch,:,:] = pred\n",
    "    b += 1\n",
    "    true = batch[1]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_compressed_10squares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "NPLOT = 3\n",
    "for kk in range(NPLOT):\n",
    "    k = random.randint(0, test_batch)\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 1)\n",
    "    plt.imshow(batch[1][k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); clim = plt.gci().get_clim();\n",
    "    if kk == 0:\n",
    "        plt.title('Exact', color='red')\n",
    "\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 2)\n",
    "    plt.imshow(pred[k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "    if kk == 0:\n",
    "        plt.title('Pred', color='red')\n",
    "\n",
    "    plt.subplot(NPLOT, 3, kk*NPLOT + 3)\n",
    "    plt.imshow(batch[1][k,:,:]-pred[k,:,:])\n",
    "    plt.xticks([]); plt.yticks([]); plt.gci().set_clim(clim);\n",
    "    if kk == 0:\n",
    "        plt.title('Error', color='red')                \n",
    "plt.show()\n",
    "#fig.savefig('compressed10squares.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "jaxisp-env",
   "language": "python",
   "name": "jaxisp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
