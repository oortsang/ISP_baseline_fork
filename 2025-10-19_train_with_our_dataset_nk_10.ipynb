{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed164b70-fb54-4fd4-936c-a0f2e2354121",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3224e2-b6f3-49c6-a2dd-b02427ddde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import time\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n",
    "\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import sparse\n",
    "jax_device = jax.devices(\"gpu\")[0]\n",
    "jax.config.update(\"jax_default_device\", jax_device)\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec49c61-82cd-4100-8c23-c15163b25784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /share/data/willett-group/oortsang/miniconda/envs/jaxisp-env/lib/python3.11/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ISP_baseline.src import models, trainers, utils\n",
    "from ISP_baseline.models import Uncompressed \n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import metrics\n",
    "from pysteps.utils.spectral import rapsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa0efa9-4223-4100-95d0-cee9bef10851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use with our MFISNet-style dataset\n",
    "import os\n",
    "from ISP_baseline.src.data_io import (\n",
    "    load_hdf5_to_dict,\n",
    "    load_cart_multifreq_dataset,\n",
    "    load_single_dir_slice,\n",
    "    load_multi_dir_slice,\n",
    "    get_multifreq_dset_dirs,\n",
    ")\n",
    "from ISP_baseline.src.datasets import (\n",
    "    convert_mfisnet_data_dict,\n",
    "    setup_tf_dataset,\n",
    "    get_io_mean_std,\n",
    ")\n",
    "from ISP_baseline.src.more_metrics import (\n",
    "    l2_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802b2e78-5208-4d2c-8e36-fb333ab4a158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761175243.314895 1423549 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abdd75-21fc-41fb-9c42-f59498a696b6",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b885f2d4-dfbb-4189-8c87-7e49a3587b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlc_repo = os.path.join(\"/\", \"home-nfs\", \"oortsang\", \"rlc-repo\")\n",
    "dataset_dir = os.path.join(rlc_repo, \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d81e86-55b9-4e42-959c-92554087e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4 # number of levels (even number)\n",
    "s = 12 # leaf size for N_x = 192\n",
    "# s = 5 # leaf size\n",
    "# s = 6\n",
    "# r = 3 # rank -- not used??\n",
    "\n",
    "downsample_ratio = 1\n",
    "s = s // downsample_ratio\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Batch size.\n",
    "batch_size = 16\n",
    "\n",
    "# Number of training datapoints.\n",
    "# NTRAIN = 21000\n",
    "# NTRAIN = 2000 # Reduced for debugging purposes\n",
    "NTRAIN = 1000 # for debugging purposes\n",
    "NVAL   = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc310b0c-1ab8-47d4-977b-8f49c1a4be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbar_str_list = [\"2.5\", \"5\", \"10\"]\n",
    "# kbar_str_list = [\"2\", \"5\", \"10\"] # uuhhh I haven't prepared the 2.5 data\n",
    "kbar_str_list = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "nk = len(kbar_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efc367a-135f-443a-a83a-ea18202c2cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: x_vals(192,), q_cart(1000, 192, 192), sample_completion(1000,), d_rs(1000, 10, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "train_dirs = get_multifreq_dset_dirs(\n",
    "    \"train\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "train_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    train_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NTRAIN,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in train_mfisnet_dd.items()])}\")\n",
    "train_wb_dd = convert_mfisnet_data_dict(\n",
    "    train_mfisnet_dd,\n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio,\n",
    ")\n",
    "# eta     = train_wb_dd[\"eta\"]\n",
    "# scatter = train_wb_dd[\"scatter\"]\n",
    "\n",
    "train_eta     = train_wb_dd[\"eta\"]\n",
    "train_scatter = train_wb_dd[\"scatter\"]\n",
    "# Get mean/std for each\n",
    "(\n",
    "    train_scatter_mean,\n",
    "    train_scatter_std,\n",
    "    train_eta_mean,\n",
    "    train_eta_std\n",
    ") = get_io_mean_std(train_scatter, train_eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c01ca3-aec6-471a-84d0-6252928e3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_dloader = setup_tf_dataset(\n",
    "    train_eta,\n",
    "    train_scatter,\n",
    "    batch_size=batch_size,\n",
    "    repeats=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852a9d28-a656-4155-babe-ae87c9a5a833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: x_vals(192,), q_cart(1000, 192, 192), sample_completion(1000,), d_rs(1000, 10, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "val_dirs = get_multifreq_dset_dirs(\n",
    "    \"val\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "val_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    val_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NVAL,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in val_mfisnet_dd.items()])}\")\n",
    "val_wb_dd = convert_mfisnet_data_dict(\n",
    "    val_mfisnet_dd,\n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio,\n",
    ")\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "val_eta     = val_wb_dd[\"eta\"]\n",
    "val_scatter = val_wb_dd[\"scatter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0363485f-b93b-4d35-9928-1eee1e7a6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset, val_dloader = setup_tf_dataset(\n",
    "    val_eta,\n",
    "    val_scatter,\n",
    "    batch_size=16,\n",
    "    repeats=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8844ed2-507d-4ade-88c5-e8973cde4833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9921b616-aa69-4513-8c79-5b1dd9ed09c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 192, 192), (1000, 2, 36864, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eta.shape, train_scatter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1c82e-62c1-4fc8-9925-2cda81b1d1d9",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf578e5-79e7-4719-939e-76bb60252a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_cnn_layers = 9\n",
    "# N_cnn_channels = 6\n",
    "N_cnn_channels = 20\n",
    "# kernel_size = 3\n",
    "kernel_size = 5\n",
    "io_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d44ab0-c9a6-4852-8547-18935d0f9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.load_mats_from_fp(os.path.join(\"tmp\", \"cart_and_rot_mats\", f\"mats_neta{neta}_nx{nx}.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b253d55-9982-4f3e-8d7c-1ad095a1ddef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2025-10-22 18:21:18.928378...\n",
      "CPU times: user 61.5 ms, sys: 211 ms, total: 272 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "print(f\"Starting at {datetime.now()}...\")\n",
    "cart_mat, r_index = utils.load_or_create_mats(\n",
    "    neta,\n",
    "    nx,\n",
    "    mats_dir=os.path.join(\"tmp\", \"cart_and_rot_mats\"),\n",
    "    mats_format=\"mats_neta{0}_nx{1}.npz\",\n",
    "    save_if_created=True,\n",
    ")\n",
    "\n",
    "# cart_mat = utils.SparsePolarToCartesian(neta, nx)\n",
    "# r_index  = utils.rotationindex(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfeae054-f6f8-48f1-b9b6-b6cd1bd96145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tornado, asyncio\n",
    "# print(tornado.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bdbdb02-1def-4feb-bc00-3b22d43a26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c49201-74e3-4b6e-8d94-6cc84ebd9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_module = Uncompressed.UncompressedModelFlexible(\n",
    "    nx = nx,\n",
    "    neta = neta,\n",
    "    cart_mat = cart_mat,\n",
    "    r_index = r_index,\n",
    "    # New parameters\n",
    "    nk=nk,\n",
    "    N_cnn_layers=N_cnn_layers,\n",
    "    N_cnn_channels=N_cnn_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    # I/O Normalization?\n",
    "    in_norm=io_norm,\n",
    "    out_norm=io_norm,\n",
    "    in_mean=jnp.array(train_scatter_mean),\n",
    "    in_std=jnp.array(train_scatter_std),\n",
    "    out_mean=jnp.array(train_eta_mean),\n",
    "    out_std=jnp.array(train_eta_std),\n",
    ")\n",
    "\n",
    "# core_module = Uncompressed.UncompressedModel(\n",
    "#     nx = nx,\n",
    "#     neta = neta,\n",
    "#     cart_mat = cart_mat,\n",
    "#     r_index = r_index,\n",
    "#     # # Doesn't support these\n",
    "#     # nk=nk,\n",
    "#     # N_cnn_layers=N_cnn_layers,\n",
    "#     # N_cnn_channels=N_cnn_channels,\n",
    "#     # kernel_size=kernel_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bebee1c-19b6-4bd3-b5b6-e06606f106d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = models.DeterministicModel(\n",
    "    input_shape = train_scatter[0].shape,\n",
    "    core_module = core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f137de2-648d-412f-89e7-f39d048ad8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:21:21.977662: W external/xla/xla/stream_executor/cuda/subprocess_compilation.cc:237] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 12.0\n",
      "2025-10-22 18:21:21.977673: W external/xla/xla/stream_executor/cuda/subprocess_compilation.cc:240] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2025-10-22 18:21:21.977713: W external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc:135] UNIMPLEMENTED: /usr/local/cuda/bin/ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 3374411\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = Model.initialize(rng)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print('Number of trainable parameters:', param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14a42193-62e2-409d-a820-cd33a7908704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 36864, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scatter[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bac17a7-2a9d-4308-9c1f-f7cb357abe7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jax.experimental.sparse.bcoo.BCOO"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cart_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48356746-9108-405d-9465-8049869dce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527d33e-3f98-44cb-a242-05dfbe50bf7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d22872e4-f1f9-4878-92c8-f534a5ab6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "num_train_steps = NTRAIN * epochs // 16  #@param\n",
    "workdir = os.path.abspath('') + f\"/tmp/2025-10-19_Uncompressed_nx_{nx}_nk_{nk}\"  #@param\n",
    "if os.path.exists(workdir):\n",
    "    import shutil\n",
    "    shutil.rmtree(workdir)\n",
    "\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ckpt_interval = 2000  #@param\n",
    "max_ckpt_to_keep = 3  #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0866784c-26de-4783-be04-a6d0705aa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.DeterministicTrainer(\n",
    "    model=Model,\n",
    "    rng=jax.random.PRNGKey(42),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12394615-6475-4c04-a710-085dd7c47ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trainer.train_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f30d0c3f-0de0-4787-9069-3ace98e11228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage associated with a single forward pass...\n",
      "RAM Used (MB): 68189\n",
      "VRAM (MB): 87439 free of 87532 total (within preallocation); usage is currently 92 and peaked at 297\n",
      "RAM Used (MB): 68328\n",
      "VRAM (MB): 87437 free of 87532 total (within preallocation); usage is currently 95 and peaked at 3136\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage associated with a single forward pass...\")\n",
    "_ = utils.get_memory_info_jax(device=jax_device, print_msg=True)\n",
    "# pre-run:     244\n",
    "# after 16:   2694\n",
    "# after 32:   5286\n",
    "# after 64:  10469\n",
    "# after 128: 20865\n",
    "trial_batch_size = 16\n",
    "\n",
    "# first_batch = next(train_dloader)\n",
    "trial_input = train_wb_dd[\"scatter\"][:trial_batch_size]\n",
    "trial_output = untrained_inference_fn(trial_input)\n",
    "\n",
    "_ = utils.get_memory_info_jax(device=jax_device, print_msg=True)\n",
    "\n",
    "# Okay, so running the forward pass is not a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0702fbf0-9a6c-490b-bf30-beb47acc5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asldkfjsdlkjfiowf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a904f8c1-995e-46d4-b640-69e3907ce5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ad8bccd08b44dcb7956f4e05cd6275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31250 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:22:23.239677: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.14 = (f32[20,5,5,90]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35574, %bitcast.35576), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_4/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:23.369583: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.130012359s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.14 = (f32[20,5,5,90]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35574, %bitcast.35576), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_4/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:24.728354: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.15 = (f32[20,5,5,110]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35698, %bitcast.35700), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_5/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:25.154253: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.425994806s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.15 = (f32[20,5,5,110]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35698, %bitcast.35700), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_5/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:26.578901: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.16 = (f32[20,5,5,130]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35822, %bitcast.35824), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_6/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:27.102570: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.52374893s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.16 = (f32[20,5,5,130]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35822, %bitcast.35824), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_6/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:28.564459: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.17 = (f32[20,5,5,150]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35946, %bitcast.35948), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_7/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:29.696936: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.132583745s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.17 = (f32[20,5,5,150]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.35946, %bitcast.35948), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_7/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:31.217382: E external/xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f32[20,5,5,170]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.36070, %bitcast.36072), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_8/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-10-22 18:22:33.007273: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.789987403s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bw-filter.18 = (f32[20,5,5,170]{3,2,1,0}, u8[0]{0}) custom-call(%bitcast.36070, %bitcast.36072), window={size=5x5 pad=2_2x2_2}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_name=\"jit(_train_step)/jit(main)/transpose(jvp(UncompressedModelFlexible))/convs_8/conv_general_dilated\" source_file=\"/share/data/willett-group/oortsang/ISP_baseline_fork/ISP_baseline/models/Uncompressed.py\" source_line=200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM Used (MB): 195750\n",
      "VRAM (MB): 87350 free of 87532 total (within preallocation); usage is currently 182 and peaked at 40887\n"
     ]
    }
   ],
   "source": [
    "_ = utils.get_memory_info_jax(device=jax_device, print_msg=True)\n",
    "\n",
    "try:\n",
    "    eval_dloader = train_dloader\n",
    "    templates.run_train(\n",
    "        train_dataloader=train_dloader,\n",
    "        trainer=trainer,\n",
    "        workdir=workdir,\n",
    "        total_train_steps=num_train_steps,\n",
    "        metric_writer=metric_writers.create_default_writer(\n",
    "            workdir, asynchronous=False\n",
    "        ),\n",
    "        metric_aggregation_steps=10,\n",
    "        eval_dataloader=eval_dloader,\n",
    "        eval_every_steps = 100,\n",
    "        num_batches_per_eval = 1,\n",
    "        callbacks=(\n",
    "            templates.TqdmProgressBar(\n",
    "                total_train_steps=num_train_steps,\n",
    "                train_monitors=(\"train_loss\",),\n",
    "                eval_monitors=(\"eval_rrmse_mean\", \"eval_rel_l2_mean\"),\n",
    "            ),\n",
    "            templates.TrainStateCheckpoint(\n",
    "                base_dir=workdir,\n",
    "                options=ocp.CheckpointManagerOptions(\n",
    "                    save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "except:\n",
    "    # Print out memory info on failure...\n",
    "    _ = utils.get_memory_info_jax(device=jax_device, print_msg=True)\n",
    "\n",
    "\n",
    "_ = utils.get_memory_info_jax(device=jax_device, print_msg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb202d0-07f8-462f-8a10-7cee8d30dca5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41be1aeb-2023-45d9-a9b0-c045968ba402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n"
     ]
    }
   ],
   "source": [
    "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
    "    f\"{workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "inference_fn = trainers.DeterministicTrainer.build_inference_fn(\n",
    "    trained_state, core_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ff7d13a-fba7-4556-ab56-24315fe0db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NTEST = 1000\n",
    "\n",
    "test_dirs = get_multifreq_dset_dirs(\n",
    "    \"test\",\n",
    "    kbar_str_list,\n",
    "    base_dir=dataset_dir,\n",
    "    dir_fmt=\"{0}_measurements_nu_{1}\"\n",
    ")\n",
    "\n",
    "test_mfisnet_dd = load_cart_multifreq_dataset(\n",
    "    test_dirs,\n",
    "    global_idx_start=0,\n",
    "    global_idx_end=NTEST,\n",
    ")\n",
    "print(f\"Loaded: {', '.join([f'{key}{val.shape}' for (key, val) in test_mfisnet_dd.items()])}\")\n",
    "test_wb_dd = convert_mfisnet_data_dict(\n",
    "    test_mfisnet_dd, \n",
    "    blur_sigma=blur_sigma,\n",
    "    downsample_ratio=downsample_ratio\n",
    ")\n",
    "\n",
    "# Try downsampling since the sparsepolartocartesian step is so slow :((\n",
    "test_eta     = test_wb_dd[\"eta\"] # [..., ::2, ::2]\n",
    "test_scatter = test_wb_dd[\"scatter\"] # [..., ::2, ::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0728c6-9803-4ee5-a349-f8807e78d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 20\n",
    "test_dataset, test_dloader = setup_tf_dataset(\n",
    "    test_eta,\n",
    "    test_scatter,\n",
    "    # val_eta,\n",
    "    # val_scatter,\n",
    "    batch_size=test_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e3cfa-b713-4b2f-9bdd-d4565ab42e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors_rrmse = []\n",
    "validation_errors_rel_l2 = []\n",
    "validation_errors_rapsd = []\n",
    "pred_eta = np.zeros(test_eta.shape)\n",
    "\n",
    "rrmse = functools.partial(\n",
    "    metrics.mean_squared_error,\n",
    "    sum_axes=(-1, -2),\n",
    "    relative=True,\n",
    "    squared=False,\n",
    ")\n",
    "rel_l2 = functools.partial(\n",
    "    l2_error,\n",
    "    l2_axes=(-1, -2),\n",
    "    relative=True,\n",
    "    squared=False,\n",
    ")\n",
    "\n",
    "# for b, batch in enumerate(test_dloader):\n",
    "for b, batch in enumerate(val_dloader):\n",
    "    # pred = inference_fn(batch[0])\n",
    "    pred = inference_fn(batch[\"scatter\"])\n",
    "    batch_slice = np.s_[b*test_batch_size: (b+1)*test_batch_size, :, :]\n",
    "    pred_eta[batch_slice] = pred\n",
    "    true = batch[\"eta\"]\n",
    "    validation_errors_rrmse.append(rrmse(pred=pred, true=true))\n",
    "    validation_errors_rel_l2.append(rel_l2(pred=pred, true=true))\n",
    "    for i in range(true.shape[0]):\n",
    "        validation_errors_rapsd.append(np.abs(np.log(rapsd(pred[i],fft_method=np.fft)/rapsd(true[i],fft_method=np.fft))))\n",
    "\n",
    "print(f\"Mean rel l2 error: {np.mean(validation_errors_rel_l2)*100:.3f}%\")\n",
    "print('relative root-mean-square error = %.3f' % (np.mean(validation_errors_rrmse)*100), '%') \n",
    "print('mean energy log ratio = %.3f' % np.mean(validation_errors_rapsd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7008cb5-e3bd-4c74-82c7-7cdc21b49fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb067c7-765d-453e-b605-e9bdc3f50417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = os.path.abspath('../..') + '/data/10hsquares_testdata'\n",
    "# test_data_path = os.path.join(\"data\", \"testdata\")\n",
    "\n",
    "# with h5py.File(f'{test_data_path}/eta.h5', 'r') as f:\n",
    "#     # Read eta data, apply Gaussian blur, and reshape\n",
    "#     eta_re = f[list(f.keys())[0]][:, :].reshape(-1, neta, neta)\n",
    "#     blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "#     eta_test = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "    \n",
    "# # Loading and preprocessing scatter data (Lambda)\n",
    "# # with h5py.File(f'{test_data_path}/scatter_order_8.h5', 'r') as f:\n",
    "# with h5py.File(f'{test_data_path}/scatter.h5', 'r') as f:\n",
    "#     keys = natsort.natsorted(f.keys())\n",
    "\n",
    "#     # Process real part of scatter data\n",
    "#     tmp1 = f[keys[3]][:, :]\n",
    "#     tmp2 = f[keys[4]][:, :]\n",
    "#     tmp3 = f[keys[5]][:, :]\n",
    "#     scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "#     # Process imaginary part of scatter data\n",
    "#     tmp1 = f[keys[0]][:, :]\n",
    "#     tmp2 = f[keys[1]][:, :]\n",
    "#     tmp3 = f[keys[2]][:, :]\n",
    "#     scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "#     # Combine real and imaginary parts\n",
    "#     scatter_test = np.stack((scatter_re, scatter_im), axis=1).astype('float32')\n",
    "    \n",
    "# # Clean up temporary variables to free memory\n",
    "# del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c916a4-790c-4dd5-b205-00d18555a72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eed1d5-22f7-4b2f-a289-ad382e384903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9a0d4-6781-4481-8d15-331f02b06641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxisp-env",
   "language": "python",
   "name": "jaxisp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
